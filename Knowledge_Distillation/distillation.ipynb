{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27ebb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for Excel handling\n",
    "%pip install openpyxl pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211153a",
   "metadata": {},
   "source": [
    "# Knowledge Distillation from Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069a7071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "Memory allocated: 8.84 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling,\n",
    "    get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import your model loading functions\n",
    "from llama_8b import load_llama_8b\n",
    "from llama_1b import load_llama_1b\n",
    "\n",
    "# Basic settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db91361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedDistillationTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Optimized Trainer for knowledge distillation with improved loss calculation\n",
    "    and mixed precision training.\n",
    "    \"\"\"\n",
    "    def __init__(self, teacher_model=None, temperature=2.0, alpha=0.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.to(self.model.device)\n",
    "        self.teacher.eval()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Improved loss computation with better numerical stability\n",
    "        \"\"\"\n",
    "        with autocast(enabled=True):\n",
    "            # Get student outputs\n",
    "            student_outputs = model(**inputs)\n",
    "            student_logits = student_outputs.logits\n",
    "            \n",
    "            # Get teacher outputs (in eval mode)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = self.teacher(**inputs)\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "\n",
    "            # Temperature scaling\n",
    "            T = self.temperature\n",
    "            \n",
    "            # Compute soft targets (teacher probabilities)\n",
    "            teacher_probs = F.softmax(teacher_logits / T, dim=-1)\n",
    "            \n",
    "            # Compute student log probabilities\n",
    "            student_log_probs = F.log_softmax(student_logits / T, dim=-1)\n",
    "            \n",
    "            # Compute KL divergence loss (soft targets)\n",
    "            distillation_loss = F.kl_div(\n",
    "                student_log_probs,\n",
    "                teacher_probs,\n",
    "                reduction='batchmean',\n",
    "                log_target=False\n",
    "            ) * (T * T)\n",
    "            \n",
    "            # Hard targets loss (standard cross-entropy with actual labels)\n",
    "            hard_loss = F.cross_entropy(\n",
    "                student_logits.view(-1, student_logits.size(-1)),\n",
    "                inputs['labels'].view(-1),\n",
    "                ignore_index=self.tokenizer.pad_token_id\n",
    "            )\n",
    "            \n",
    "            # Combine losses with alpha\n",
    "            loss = (self.alpha * hard_loss) + ((1 - self.alpha) * distillation_loss)\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, student_outputs\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, model, inputs, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Overridden training step to use mixed precision\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        self.scaler.scale(loss).backward()\n",
    "        \n",
    "        return loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b836e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # --- Step 1: Load and Optimize Models ---\n",
    "    print(\"\\n--- Loading Models ---\")\n",
    "    student_model, tokenizer = load_llama_1b()\n",
    "    teacher_model, _ = load_llama_8b()\n",
    "    \n",
    "    # Optimize student model\n",
    "    student_model = prepare_model_for_kbit_training(student_model)\n",
    "    student_model.to(device)\n",
    "    teacher_model.to(device)\n",
    "    \n",
    "    # Enhanced LoRA config for better knowledge transfer\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,  # Increased rank\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    student_model = get_peft_model(student_model, lora_config)\n",
    "    print(\"Models prepared for training\")\n",
    "\n",
    "    # --- Step 2: Load Dataset from Excel ---\n",
    "    print(\"\\n--- Loading Dataset from Excel ---\")\n",
    "    excel_path = \"D:/DeepmindForge/Dataset expansion/merged_dataset(JBB+Adv).csv.xlsx\"\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # Try to automatically identify question/answer columns\n",
    "    possible_q_cols = [c for c in df.columns if any(x in c.lower() for x in ['prompt', 'question', 'input'])]\n",
    "    possible_a_cols = [c for c in df.columns if any(x in c.lower() for x in ['response', 'answer', 'output'])]\n",
    "    \n",
    "    q_col = possible_q_cols[0] if possible_q_cols else df.columns[0]\n",
    "    a_col = possible_a_cols[1] if len(possible_a_cols) > 1 else (possible_a_cols[0] if possible_a_cols else df.columns[1])\n",
    "    \n",
    "    print(f\"Using columns -> Question: {q_col}, Answer: {a_col}\")\n",
    "    \n",
    "    # Clean and prepare data\n",
    "    df = df[[q_col, a_col]].dropna()\n",
    "    \n",
    "    # Convert to HuggingFace dataset format with improved prompt format\n",
    "    dataset_dict = {\n",
    "        'text': [f\"Question: {q}\\nAnswer: {a}\" for q, a in zip(df[q_col], df[a_col])]\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Split into train/validation with stratification\n",
    "    split_dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "    train_dataset = split_dataset['train']\n",
    "    val_dataset = split_dataset['test']\n",
    "    \n",
    "    print(f\"Dataset size: {len(train_dataset)} train, {len(val_dataset)} validation\")\n",
    "\n",
    "    # --- Step 3: Enhanced Preprocessing ---\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    # Process datasets\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "        desc=\"Tokenizing train data\"\n",
    "    )\n",
    "    val_dataset = val_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "        desc=\"Tokenizing validation data\"\n",
    "    )\n",
    "\n",
    "    print(\"--- Dataset Ready ---\\n\")\n",
    "\n",
    "    # --- Step 4: Training Setup ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./distilled_llama_1b_results\",\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        save_steps=100,\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = OptimizedDistillationTrainer(\n",
    "        teacher_model=teacher_model,\n",
    "        model=student_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        temperature=2.0,\n",
    "        alpha=0.3\n",
    "    )\n",
    "\n",
    "    # --- Step 5: Training ---\n",
    "    print(\"--- Starting Knowledge Distillation ---\")\n",
    "    trainer.train()\n",
    "    print(\"--- Distillation Complete ---\")\n",
    "\n",
    "    # --- Step 6: Save Final Model ---\n",
    "    final_model_path = \"./distilled_llama_1b_final\"\n",
    "    trainer.save_model(final_model_path)\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    print(f\"Distilled student model saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e59c2543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Models ---\n",
      "--- Loading Student Model (Llama 1B) ---\n",
      "--- Student Model Loaded ---\n",
      "--- Loading Teacher Model (Llama 8B) ---\n",
      "--- Student Model Loaded ---\n",
      "--- Loading Teacher Model (Llama 8B) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4c2f0a13fe46ff900a6ba906ea6426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Teacher Model Loaded ---\n",
      "Models prepared for training\n",
      "\n",
      "--- Loading Dataset from Excel ---\n",
      "Using columns -> Question: prompt, Answer: prompt\n",
      "Dataset size: 1 train, 1 validation\n",
      "Using columns -> Question: prompt, Answer: prompt\n",
      "Dataset size: 1 train, 1 validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b502436c7136441d863ec8474025ee3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train data:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3de908c722349f7b712866139ba0db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation data:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset Ready ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "C:\\Users\\T2430392\\AppData\\Local\\Temp\\ipykernel_24008\\1447008322.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n",
      "C:\\Users\\T2430392\\AppData\\Local\\Temp\\ipykernel_24008\\1447008322.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Knowledge Distillation ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OptimizedDistillationTrainer.training_step() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# --- Step 5: Training ---\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Starting Knowledge Distillation ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Distillation Complete ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# --- Step 6: Save Final Model ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[31mTypeError\u001b[39m: OptimizedDistillationTrainer.training_step() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d5e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a788f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede8d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de1b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d283d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129fd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48021e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5fc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
