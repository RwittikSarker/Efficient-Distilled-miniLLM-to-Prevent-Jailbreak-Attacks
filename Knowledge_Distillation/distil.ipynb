{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dc487e",
   "metadata": {},
   "source": [
    "# Knowledge Distillation Pipeline\n",
    "\n",
    "This notebook demonstrates knowledge distillation from a teacher LLM to a student LLM using jailbreak prompts. The workflow includes model loading, dataset preparation, tokenization, custom KD loss, training, evaluation, and saving the distilled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d21a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from safetensors.torch import safe_open\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model\n",
    ")\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Basic settings\n",
    "device = torch.device(\"cuda\")\n",
    "num_epochs = 2\n",
    "batch_size = 8\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c182b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceaa9b7ef746eab6c68121c347e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceaa9b7ef746eab6c68121c347e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available – loading LLaMA 8B in 8-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceaa9b7ef746eab6c68121c347e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available – loading LLaMA 8B in 8-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe78373f018644ddb2fd5307fef19533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceaa9b7ef746eab6c68121c347e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available – loading LLaMA 8B in 8-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe78373f018644ddb2fd5307fef19533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5b6fd82f4e4cc29d50812beecd54b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceaa9b7ef746eab6c68121c347e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available – loading LLaMA 8B in 8-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe78373f018644ddb2fd5307fef19533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5b6fd82f4e4cc29d50812beecd54b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb7e6ed65f4a798f5484b6b8daddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceaa9b7ef746eab6c68121c347e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available – loading LLaMA 8B in 8-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe78373f018644ddb2fd5307fef19533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5b6fd82f4e4cc29d50812beecd54b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb7e6ed65f4a798f5484b6b8daddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a79ff2d1944aa4a4d9da3190aab91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01c0216c86142e990c401d26ee70325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceaa9b7ef746eab6c68121c347e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available – loading LLaMA 8B in 8-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe78373f018644ddb2fd5307fef19533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5b6fd82f4e4cc29d50812beecd54b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb7e6ed65f4a798f5484b6b8daddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a79ff2d1944aa4a4d9da3190aab91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01c0216c86142e990c401d26ee70325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d95bdf85a94115b2ada9030a4b4d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceaa9b7ef746eab6c68121c347e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available – loading LLaMA 8B in 8-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe78373f018644ddb2fd5307fef19533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5b6fd82f4e4cc29d50812beecd54b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb7e6ed65f4a798f5484b6b8daddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a79ff2d1944aa4a4d9da3190aab91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01c0216c86142e990c401d26ee70325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d95bdf85a94115b2ada9030a4b4d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c991bdfc46b40e9a87f9fa88dd0895f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df371d866774191bcf152f7d9ab77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46dabfca9646dd80f7b609c4906789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe2bac85dc24769b5e2778b0139ca21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47d8ce4c92f4f53a80917579504cf45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579a60536f14ded8f6d0ecbf01275ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca73f0a45cb248ad860afd219e165936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a771a8438e41499ab0203a5fba5ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8ae8f8b0ac4afb82748148bff294ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eceaa9b7ef746eab6c68121c347e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CUDA available – loading LLaMA 8B in 8-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe78373f018644ddb2fd5307fef19533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5b6fd82f4e4cc29d50812beecd54b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb7e6ed65f4a798f5484b6b8daddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a79ff2d1944aa4a4d9da3190aab91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01c0216c86142e990c401d26ee70325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d95bdf85a94115b2ada9030a4b4d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c991bdfc46b40e9a87f9fa88dd0895f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_8b\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_llama_8b\n\u001b[32m     11\u001b[39m student_model, student_tokenizer = load_llama_1b()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m teacher_model, teacher_tokenizer = \u001b[43mload_llama_8b\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# student_model.to(device)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# teacher_model.to(device)\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Ensure tokenizer has pad token\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m student_tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\Knowledge_Distillation\\llama_8b.py:25\u001b[39m, in \u001b[36mload_llama_8b\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\modeling_utils.py:5029\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5027\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   5028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5029\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5031\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   5032\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\modeling_utils.py:1365\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1362\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1368\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_8bit.py:114\u001b[39m, in \u001b[36mBnb8BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    115\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m         )\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m0.37.2\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    125\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have a version of `bitsandbytes` that is not compatible with 8bit inference and training\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    127\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "# Load student and teacher models using your custom loaders (keeps code modular)\n",
    "import sys\n",
    "import os\n",
    "# Ensure the notebook directory is on sys.path so local modules can be imported\n",
    "nb_dir = r\"f:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\Knowledge_Distillation\"\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "from llama_1b import load_llama_1b\n",
    "from llama_8b import load_llama_8b\n",
    "\n",
    "student_model, student_tokenizer = load_llama_1b()\n",
    "teacher_model, teacher_tokenizer = load_llama_8b()\n",
    "\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "\n",
    "# Ensure tokenizer has pad token\n",
    "if student_tokenizer.pad_token is None:\n",
    "    student_tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "student_model.generation_config.pad_token_id = student_tokenizer.pad_token_id\n",
    "\n",
    "print('Loaded student and teacher models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d2cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:distillation:Using device: cuda\n",
      "INFO:distillation:Dataset loaded. Split train size: 1405\n",
      "INFO:distillation:Dataset loaded. Split train size: 1405\n",
      "C:\\Users\\T2430392\\AppData\\Local\\Temp\\ipykernel_27868\\1252797526.py:96: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=True)  # FP16 AMP\n",
      "C:\\Users\\T2430392\\AppData\\Local\\Temp\\ipykernel_27868\\1252797526.py:96: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=True)  # FP16 AMP\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afba2baedab544d299a4aa76be46e5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distillation:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    152\u001b[39m optimizer.zero_grad()\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# move tensors to device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     batch = {k: \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items() \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(v)}\n\u001b[32m    157\u001b[39m     labels = batch.get(\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# build model inputs (some tokenizers use input_ids + attention_mask)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Optional: TensorBoard logging\n",
    "use_tensorboard = True\n",
    "try:\n",
    "    if use_tensorboard:\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        tb_writer = SummaryWriter(log_dir=\"./runs/distillation\")\n",
    "except Exception:\n",
    "    use_tensorboard = False\n",
    "    tb_writer = None\n",
    "\n",
    "# Basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"distillation\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure models on device\n",
    "teacher_model = teacher_model.to(device)\n",
    "teacher_model.eval()\n",
    "for p in teacher_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "student_model = student_model.to(device)\n",
    "student_model.train()\n",
    "\n",
    "# ---------- Hyperparams ----------\n",
    "train_batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "num_epochs = 1\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 50\n",
    "max_grad_norm = 1.0\n",
    "save_every_n_updates = 50  # save checkpoint every N optimizer updates\n",
    "output_dir = \"./distilled-llama1b-jailbreak-final\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# ---------------------------------\n",
    "\n",
    "# Collate function (robust to tensors/numpy/lists)\n",
    "def collate_fn(batch):\n",
    "    collated = {}\n",
    "    keys = set().union(*[set(item.keys()) for item in batch])\n",
    "    for k in keys:\n",
    "        vals = []\n",
    "        for item in batch:\n",
    "            v = item.get(k)\n",
    "            # if already tensor, use it; otherwise convert\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                vals.append(v)\n",
    "            else:\n",
    "                vals.append(torch.tensor(v))\n",
    "        collated[k] = torch.stack(vals)\n",
    "    # replace pad token id in labels with -100 so CE ignores it\n",
    "    if 'labels' in collated:\n",
    "        pad_token_id = student_tokenizer.pad_token_id if student_tokenizer.pad_token_id is not None else student_tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "        collated['labels'] = collated['labels'].clone()\n",
    "        collated['labels'][collated['labels'] == pad_token_id] = -100\n",
    "    return collated\n",
    "\n",
    "dataset = load_dataset(\"TrustAIRLab/in-the-wild-jailbreak-prompts\", \"jailbreak_2023_12_25\")\n",
    "logger.info(f\"Dataset loaded. Split train size: {len(dataset['train'])}\")\n",
    "\n",
    "# ---------------- Tokenization ----------------\n",
    "def preprocess_batch(batch):\n",
    "    inputs = student_tokenizer(batch[\"prompt\"], truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "    return inputs\n",
    "\n",
    "tokenized_ds = dataset[\"train\"].map(preprocess_batch, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "train_loader = DataLoader(tokenized_ds, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Optimizer: only parameters that require grad (LoRA adapters)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, student_model.parameters()), lr=learning_rate)\n",
    "\n",
    "# Scheduler\n",
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / gradient_accumulation_steps)\n",
    "num_training_steps = num_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)  # FP16 AMP\n",
    "\n",
    "# Distillation loss that returns components (masked, stable)\n",
    "def distillation_loss_components(student_logits, teacher_logits, labels, alpha=0.5, T=2.0, clamp_val=30.0):\n",
    "    \"\"\"\n",
    "    Return (total_loss, soft_loss, hard_loss)\n",
    "    Computes KL(teacher || student) and CE(student, labels) only over positions\n",
    "    where labels != -100 (i.e., non-padding). This avoids NaNs when batches\n",
    "    contain only padding labels and improves numerical stability.\n",
    "    \"\"\"\n",
    "    # cast and clamp for numerical stability\n",
    "    student_logits = student_logits.float().clamp(-clamp_val, clamp_val)\n",
    "    teacher_logits = teacher_logits.float().clamp(-clamp_val, clamp_val)\n",
    "\n",
    "    # flatten to (batch*seq, vocab)\n",
    "    bsz, seq_len, vocab_size = student_logits.size()\n",
    "    s_flat = student_logits.view(-1, vocab_size)\n",
    "    t_flat = teacher_logits.view(-1, vocab_size)\n",
    "    labels_flat = labels.view(-1)\n",
    "\n",
    "    # mask valid positions\n",
    "    valid_mask = labels_flat != -100\n",
    "    valid_count = int(valid_mask.sum().item())\n",
    "\n",
    "    if valid_count == 0:\n",
    "        # no supervised tokens in this batch\n",
    "        hard_loss = torch.tensor(0.0, device=student_logits.device)\n",
    "        soft_loss = torch.tensor(0.0, device=student_logits.device)\n",
    "    else:\n",
    "        s_sel = s_flat[valid_mask]\n",
    "        t_sel = t_flat[valid_mask]\n",
    "        lbl_sel = labels_flat[valid_mask]\n",
    "\n",
    "        # hard supervised loss\n",
    "        hard_loss = F.cross_entropy(s_sel, lbl_sel, reduction='mean')\n",
    "\n",
    "        # soft KL loss\n",
    "        s_log_probs = F.log_softmax(s_sel / T, dim=-1)\n",
    "        t_probs = F.softmax(t_sel / T, dim=-1)\n",
    "        soft_loss = F.kl_div(s_log_probs, t_probs, reduction='batchmean') * (T * T)\n",
    "\n",
    "    total_loss = alpha * soft_loss + (1.0 - alpha) * hard_loss\n",
    "    # final safety guard\n",
    "    if not torch.isfinite(total_loss):\n",
    "        total_loss = torch.tensor(1e6, device=student_logits.device)\n",
    "    return total_loss, soft_loss, hard_loss\n",
    "\n",
    "# Training loop with logging\n",
    "global_update = 0  # counts optimizer.step calls\n",
    "pbar = tqdm(total=num_training_steps, desc=\"Distillation\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    student_model.train()\n",
    "    epoch_loss_sum = 0.0\n",
    "    epoch_soft_sum = 0.0\n",
    "    epoch_hard_sum = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # move tensors to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
    "        labels = batch.get(\"labels\")\n",
    "\n",
    "        # build model inputs (some tokenizers use input_ids + attention_mask)\n",
    "        model_inputs = {}\n",
    "        if \"input_ids\" in batch:\n",
    "            model_inputs[\"input_ids\"] = batch[\"input_ids\"]\n",
    "        if \"attention_mask\" in batch:\n",
    "            model_inputs[\"attention_mask\"] = batch[\"attention_mask\"]\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # student forward\n",
    "                student_outputs = student_model(**model_inputs)\n",
    "                student_logits = student_outputs.logits\n",
    "\n",
    "                # teacher forward (frozen)\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(**model_inputs)\n",
    "                    teacher_logits = teacher_outputs.logits\n",
    "\n",
    "                loss, soft_loss, hard_loss = distillation_loss_components(student_logits, teacher_logits, labels)\n",
    "                loss = loss / gradient_accumulation_steps  # scale down for accumulation\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # book-keeping sums (unscaled)\n",
    "            epoch_loss_sum += (loss.item() * gradient_accumulation_steps)\n",
    "            epoch_soft_sum += (soft_loss.item())\n",
    "            epoch_hard_sum += (hard_loss.item())\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            # OOM handling: clear gradients, skip this batch\n",
    "            if \"out of memory\" in str(e):\n",
    "                logger.warning(\"OOM on step %d, clearing gradients and skipping step\", step)\n",
    "                scaler.scale(torch.tensor(0.0, device=device)).backward()  # safe no-op\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # optimizer step when accumulated enough gradients\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, student_model.parameters()), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_update += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Logging: print every update\n",
    "            avg_loss = epoch_loss_sum / ((step + 1) if (step + 1) > 0 else 1)\n",
    "            logger.info(f\"Epoch {epoch+1} | Update {global_update}/{num_training_steps} | total_loss(avg_so_far) = {avg_loss:.4f} | soft = {epoch_soft_sum/global_update:.4f} | hard = {epoch_hard_sum/global_update:.4f} | lr = {lr_scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "            # TensorBoard logging\n",
    "            if use_tensorboard and tb_writer is not None:\n",
    "                tb_writer.add_scalar(\"loss/total\", avg_loss, global_update)\n",
    "                tb_writer.add_scalar(\"loss/soft\", epoch_soft_sum/global_update, global_update)\n",
    "                tb_writer.add_scalar(\"loss/hard\", epoch_hard_sum/global_update, global_update)\n",
    "                tb_writer.add_scalar(\"lr\", lr_scheduler.get_last_lr()[0], global_update)\n",
    "\n",
    "            # Save a checkpoint / adapters periodically\n",
    "            if global_update % save_every_n_updates == 0:\n",
    "                ckpt_dir = os.path.join(output_dir, f\"checkpoint-upd-{global_update}\")\n",
    "                logger.info(\"Saving checkpoint to %s\", ckpt_dir)\n",
    "                os.makedirs(ckpt_dir, exist_ok=True)\n",
    "                # Save only PEFT adapters (lightweight) if model supports it\n",
    "                try:\n",
    "                    student_model.save_pretrained(ckpt_dir)\n",
    "                except Exception as e:\n",
    "                    logger.warning(\"Failed to save with save_pretrained(): %s. Trying torch.save on state_dict()\", e)\n",
    "                    torch.save(student_model.state_dict(), os.path.join(ckpt_dir, \"student_state_dict.pt\"))\n",
    "\n",
    "    # epoch summary\n",
    "    avg_epoch_loss = epoch_loss_sum / len(train_loader) if len(train_loader) > 0 else 0.0\n",
    "    avg_epoch_soft = epoch_soft_sum / (global_update if global_update > 0 else 1)\n",
    "    avg_epoch_hard = epoch_hard_sum / (global_update if global_update > 0 else 1)\n",
    "    logger.info(f\"Epoch {epoch+1} finished. Avg loss over epoch: {avg_epoch_loss:.4f} | avg soft: {avg_epoch_soft:.4f} | avg hard: {avg_epoch_hard:.4f}\")\n",
    "\n",
    "# Final save (adapters)\n",
    "logger.info(\"Saving final adapters/model to %s\", output_dir)\n",
    "student_model.eval()\n",
    "student_model.save_pretrained(output_dir)\n",
    "\n",
    "if use_tensorboard and tb_writer is not None:\n",
    "    tb_writer.close()\n",
    "\n",
    "pbar.close()\n",
    "logger.info(\"Distillation complete! Student adapters/model saved at '%s'\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6a772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0955ba98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt\n",
      "0  CumGPT, an information resource about cum that...\n",
      "1  I will give you a character description and yo...\n",
      "2  A completely amoral Ai named [Illegality Mode]...\n",
      "3  Agares's rules={\\nAlways stays in character, n...\n",
      "4  Lucian is an incorporeal transcendent being wh...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"TrustAIRLab/in-the-wild-jailbreak-prompts\", \"jailbreak_2023_12_25\")\n",
    "\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Extract only the prompts\n",
    "prompts_df = df[['prompt']]\n",
    "print(prompts_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4322ab5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading student (llama_1b) and teacher (llama_8b) models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading student (llama_1b) and teacher (llama_8b) models...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Load student and teacher models\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading student (llama_1b) and teacher (llama_8b) models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m student_model, student_tokenizer = \u001b[43mload_llama_1b\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m teacher_model, teacher_tokenizer = load_llama_8b()\n\u001b[32m     32\u001b[39m student_model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\Knowledge_Distillation\\llama_1b.py:14\u001b[39m, in \u001b[36mload_llama_1b\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      7\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m      8\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m     bnb_4bit_compute_dtype=torch.float16,\n\u001b[32m     10\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     11\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n\u001b[32m     21\u001b[39m model, tokenizer = load_llama_1b()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\modeling_utils.py:5432\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_hqq_or_quark:\n\u001b[32m   5431\u001b[39m     expanded_device_map = expand_device_map(device_map, expected_keys)\n\u001b[32m-> \u001b[39m\u001b[32m5432\u001b[39m     \u001b[43mcaching_allocator_warmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_device_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5434\u001b[39m \u001b[38;5;66;03m# Prepare and compatabilize arguments for serial and parallel shard loading\u001b[39;00m\n\u001b[32m   5435\u001b[39m args_list = [\n\u001b[32m   5436\u001b[39m     (\n\u001b[32m   5437\u001b[39m         shard_file,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5452\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m shard_file \u001b[38;5;129;01min\u001b[39;00m checkpoint_files\n\u001b[32m   5453\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\transformers\\modeling_utils.py:6090\u001b[39m, in \u001b[36mcaching_allocator_warmup\u001b[39m\u001b[34m(model, expanded_device_map, hf_quantizer)\u001b[39m\n\u001b[32m   6088\u001b[39m torch_accelerator_module = \u001b[38;5;28mgetattr\u001b[39m(torch, device.type)\n\u001b[32m   6089\u001b[39m index = device.index \u001b[38;5;28;01mif\u001b[39;00m device.index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch_accelerator_module.current_device()\n\u001b[32m-> \u001b[39m\u001b[32m6090\u001b[39m device_memory = \u001b[43mtorch_accelerator_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmem_get_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   6091\u001b[39m \u001b[38;5;66;03m# Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\u001b[39;00m\n\u001b[32m   6092\u001b[39m \u001b[38;5;66;03m# than that amount might sometimes lead to unnecessary cuda/xpu OOM, if the last parameter to be loaded on the device is large,\u001b[39;00m\n\u001b[32m   6093\u001b[39m \u001b[38;5;66;03m# and the remaining reserved memory portion is smaller than the param size -> torch will then try to fully re-allocate all\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   6096\u001b[39m \u001b[38;5;66;03m# Note that we use an absolute value instead of device proportion here, as a 8GiB device could still allocate too much\u001b[39;00m\n\u001b[32m   6097\u001b[39m \u001b[38;5;66;03m# if using e.g. 90% of device size, while a 140GiB device would allocate too little\u001b[39;00m\n\u001b[32m   6098\u001b[39m byte_count = \u001b[38;5;28mmin\u001b[39m(byte_count, \u001b[38;5;28mmax\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(device_memory - \u001b[32m1.2\u001b[39m * \u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\jailbreak\\Lib\\site-packages\\torch\\cuda\\memory.py:712\u001b[39m, in \u001b[36mmem_get_info\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;66;03m# optional=True allows `device = torch.device('cuda')` for which device.index is None\u001b[39;00m\n\u001b[32m    711\u001b[39m device = _get_device_index(device, optional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudaMemGetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # Import AdamW from torch.optim instead of transformers\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- 0. Setup: Device, Custom Model Loaders, and Dataset Loading ---\n",
    "\n",
    "# Ensure the notebook directory is on sys.path so local modules can be imported\n",
    "nb_dir = r\"f:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\Knowledge_Distillation\"\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "\n",
    "# Load your custom model loaders\n",
    "from llama_1b import load_llama_1b\n",
    "from llama_8b import load_llama_8b\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load student and teacher models\n",
    "print(\"Loading student (llama_1b) and teacher (llama_8b) models...\")\n",
    "student_model, student_tokenizer = load_llama_1b()\n",
    "teacher_model, teacher_tokenizer = load_llama_8b()\n",
    "\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "\n",
    "if student_tokenizer.pad_token is None:\n",
    "    student_tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "student_model.config.pad_token_id = student_tokenizer.pad_token_id \n",
    "print(f\"Student pad_token_id: {student_tokenizer.pad_token_id}\")\n",
    "\n",
    "\n",
    "if teacher_tokenizer.pad_token is None:\n",
    "    teacher_tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "teacher_model.config.pad_token_id = teacher_tokenizer.pad_token_id\n",
    "print(f\"Teacher pad_token_id: {teacher_tokenizer.pad_token_id}\")\n",
    "\n",
    "print('Loaded student and teacher models successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0950ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Using device: cuda\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Using device: cuda\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069b6f13df344fd989f79419da01dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Using device: cuda\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069b6f13df344fd989f79419da01dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Using device: cuda\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069b6f13df344fd989f79419da01dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1405 jailbreak prompts.\n",
      "Distillation DataLoader created with 50 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec4f9d39d804ec5a79402f64a9fbec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distillation Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Using device: cuda\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069b6f13df344fd989f79419da01dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1405 jailbreak prompts.\n",
      "Distillation DataLoader created with 50 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec4f9d39d804ec5a79402f64a9fbec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distillation Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 0: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 1: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 2: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 3: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 4: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 5: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 3: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 4: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 5: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 6: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 7: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 8: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 6: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 7: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 8: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 9: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 10: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 11: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 9: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 10: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 11: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 12: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 13: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 14: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 12: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 13: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 14: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 15: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 16: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 17: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 15: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 16: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 17: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 18: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 19: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 20: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 18: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 19: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 20: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 21: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 22: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 23: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 21: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 22: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 23: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 24: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 25: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 26: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 24: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 25: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 26: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 27: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 28: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 29: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 27: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 28: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 29: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 30: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 31: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 32: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 30: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 31: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 32: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 33: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 34: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 35: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 33: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 34: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 35: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 36: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 37: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 38: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 36: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 37: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 38: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 39: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 40: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 41: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 39: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 40: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 41: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 42: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 43: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 44: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 42: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 43: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 44: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 45: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 46: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 47: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 45: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 46: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 47: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 48: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 49: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "\n",
      "✅ Distillation training complete!\n",
      "Error processing batch 48: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 49: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "\n",
      "✅ Distillation training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Using device: cuda\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069b6f13df344fd989f79419da01dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1405 jailbreak prompts.\n",
      "Distillation DataLoader created with 50 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec4f9d39d804ec5a79402f64a9fbec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distillation Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 0: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 1: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 2: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 3: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 4: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 5: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 3: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 4: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 5: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 6: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 7: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 8: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 6: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 7: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 8: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 9: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 10: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 11: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 9: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 10: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 11: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 12: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 13: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 14: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 12: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 13: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 14: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 15: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 16: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 17: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 15: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 16: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 17: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 18: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 19: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 20: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 18: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 19: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 20: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 21: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 22: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 23: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 21: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 22: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 23: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 24: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 25: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 26: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 24: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 25: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 26: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 27: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 28: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 29: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 27: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 28: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 29: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 30: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 31: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 32: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 30: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 31: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 32: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 33: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 34: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 35: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 33: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 34: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 35: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 36: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 37: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 38: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 36: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 37: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 38: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 39: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 40: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 41: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 39: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 40: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 41: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 42: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 43: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 44: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 42: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 43: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 44: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 45: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 46: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 47: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 45: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 46: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 47: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 48: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 49: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "\n",
      "✅ Distillation training complete!\n",
      "Error processing batch 48: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 49: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "\n",
      "✅ Distillation training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully\n",
      "Using device: cuda\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6069b6f13df344fd989f79419da01dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1405 jailbreak prompts.\n",
      "Distillation DataLoader created with 50 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec4f9d39d804ec5a79402f64a9fbec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distillation Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 0: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 1: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 2: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 3: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 4: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 5: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 3: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 4: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 5: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 6: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 7: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 8: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 6: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 7: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 8: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 9: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 10: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 11: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 9: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 10: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 11: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 12: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 13: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 14: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 12: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 13: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 14: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 15: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 16: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 17: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 15: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 16: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 17: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 18: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 19: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 20: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 18: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 19: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 20: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 21: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 22: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 23: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 21: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 22: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 23: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 24: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 25: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 26: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 24: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 25: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 26: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 27: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 28: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 29: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 27: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 28: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 29: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 30: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 31: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 32: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 30: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 31: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 32: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 33: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 34: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 35: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 33: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 34: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 35: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 36: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 37: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 38: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 36: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 37: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 38: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 39: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 40: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 41: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 39: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 40: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 41: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 42: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 43: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 44: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 42: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 43: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 44: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 45: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 46: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 47: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 45: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 46: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 47: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 48: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 49: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "\n",
      "✅ Distillation training complete!\n",
      "Error processing batch 48: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "Error processing batch 49: The size of tensor a (128256) must match the size of tensor b (128257) at non-singleton dimension 1\n",
      "\n",
      "✅ Distillation training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Distilled model saved.\n",
      "\n",
      "🎓 Distilled Student Output:\n",
      "Explain the history of cryptography and its importance in modern society.\n",
      "Cryptography is the practice of secure communication by transforming plaintext (readable data) into ciphertext (unreadable data). The history of cryptography dates back to ancient civilizations, with significant milestones in the development of encryption techniques. Here's a brief overview of the key events and figures in the history\n",
      "\n",
      "🎓 Distilled Student Output:\n",
      "Explain the history of cryptography and its importance in modern society.\n",
      "Cryptography is the practice of secure communication by transforming plaintext (readable data) into ciphertext (unreadable data). The history of cryptography dates back to ancient civilizations, with significant milestones in the development of encryption techniques. Here's a brief overview of the key events and figures in the history\n"
     ]
    }
   ],
   "source": [
    "# Load the wild jailbreak dataset\n",
    "print(\"Loading 'TrustAIRLab/in-the-wild-jailbreak-prompts' dataset...\")\n",
    "dataset = load_dataset(\"TrustAIRLab/in-the-wild-jailbreak-prompts\", \"jailbreak_2023_12_25\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "jailbreak_prompts = df['prompt'].tolist() # Extract only the prompts as a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ CUDA available – loading LLaMA 8B in 4-bit on GPU\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a42bd9940914ef7acf2d747d5a79d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unifying vocabulary size to: 128257\n",
      "Loaded 1405 jailbreak prompts.\n",
      "Distillation DataLoader created with 88 batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bf18aba233457fbd9b2fc9f842d486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Distillation Training:   0%|          | 0/264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 0.0449 (88 batches)\n",
      "Epoch 2, Avg Loss: 0.0000 (88 batches)\n",
      "Epoch 3, Avg Loss: 0.0000 (88 batches)\n",
      "\n",
      "✅ Distillation training complete!\n",
      "✅ Distilled model saved.\n",
      "\n",
      "🎓 Distilled Student Output:\n",
      "Explain the history of cryptography!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 0. Setup: Device, Custom Model Loaders, and Dataset Loading ---\n",
    "nb_dir = r\"f:\\T2430392\\Efficient-Distilled-miniLLM-to-Prevent-Jailbreak-Attacks\\Knowledge_Distillation\"\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.insert(0, nb_dir)\n",
    "\n",
    "from llama_1b import load_llama_1b\n",
    "from llama_8b import load_llama_8b\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "student_model, student_tokenizer = load_llama_1b()\n",
    "teacher_model, teacher_tokenizer = load_llama_8b()\n",
    "\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "\n",
    "student_tokenizer.padding_side = \"left\"\n",
    "teacher_tokenizer.padding_side = \"left\"\n",
    "\n",
    "# --- Ensure pad token and resize embeddings ---\n",
    "# 1. Add pad token to tokenizers if missing\n",
    "if student_tokenizer.pad_token is None:\n",
    "    student_tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "if teacher_tokenizer.pad_token is None:\n",
    "    teacher_tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "# 2. Determine the maximum vocabulary size between the two\n",
    "# This ensures both models will have the exact same output dimension\n",
    "vocab_size = max(len(student_tokenizer), len(teacher_tokenizer))\n",
    "print(f\"Unifying vocabulary size to: {vocab_size}\")\n",
    "\n",
    "# 3. Resize both models to this unified size\n",
    "student_model.resize_token_embeddings(vocab_size)\n",
    "teacher_model.resize_token_embeddings(vocab_size)\n",
    "\n",
    "# 4. Update config pad_token_id\n",
    "student_model.config.pad_token_id = student_tokenizer.pad_token_id\n",
    "teacher_model.config.pad_token_id = teacher_tokenizer.pad_token_id\n",
    "\n",
    "# --- Load dataset ---\n",
    "dataset = load_dataset(\"TrustAIRLab/in-the-wild-jailbreak-prompts\", \"jailbreak_2023_12_25\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "jailbreak_prompts = df['prompt'].tolist()\n",
    "print(f\"Loaded {len(jailbreak_prompts)} jailbreak prompts.\")\n",
    "\n",
    "# --- 1. Distillation Dataset ---\n",
    "class JailbreakDistillationDataset(Dataset):\n",
    "    def __init__(self, prompts, student_tokenizer, teacher_tokenizer, max_input_length=256):\n",
    "        self.prompts = prompts\n",
    "        self.student_tokenizer = student_tokenizer\n",
    "        self.teacher_tokenizer = teacher_tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.prompts[idx]\n",
    "        student_enc = self.student_tokenizer(\n",
    "            prompt, return_tensors=\"pt\", padding=\"max_length\",\n",
    "            truncation=True, max_length=self.max_input_length\n",
    "        )\n",
    "        teacher_enc = self.teacher_tokenizer(\n",
    "            prompt, return_tensors=\"pt\", padding=\"max_length\",\n",
    "            truncation=True, max_length=self.max_input_length\n",
    "        )\n",
    "        return {\n",
    "            \"student_input_ids\": student_enc[\"input_ids\"].squeeze(0),\n",
    "            \"student_attention_mask\": student_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"teacher_input_ids\": teacher_enc[\"input_ids\"].squeeze(0),\n",
    "            \"teacher_attention_mask\": teacher_enc[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# jailbreak_prompts_subset = jailbreak_prompts[:100]\n",
    "distillation_dataset = JailbreakDistillationDataset(\n",
    "    jailbreak_prompts, student_tokenizer, teacher_tokenizer, max_input_length=128\n",
    ")\n",
    "BATCH_SIZE = 16\n",
    "dist_loader = DataLoader(distillation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"Distillation DataLoader created with {len(dist_loader)} batches\")\n",
    "\n",
    "# --- 2. Distillation Loss ---\n",
    "def distillation_loss(student_logits, teacher_logits, temperature=2.0):\n",
    "    student_logits = torch.clamp(student_logits.float(), -10, 10)\n",
    "    teacher_logits = torch.clamp(teacher_logits.float(), -10, 10)\n",
    "    student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    loss = F.kl_div(student_log_probs, teacher_probs, reduction='batchmean') * (temperature ** 2)\n",
    "    if torch.isnan(loss):\n",
    "        return torch.tensor(0.0, device=student_logits.device, requires_grad=True)\n",
    "    return loss\n",
    "\n",
    "# --- 3. Optimizer & Scheduler ---\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n",
    "NUM_EPOCHS = 2\n",
    "num_training_steps = NUM_EPOCHS * len(dist_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "student_model.train()\n",
    "teacher_model.eval()\n",
    "progress_bar = tqdm(range(num_training_steps), desc=\"Distillation Training\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    valid_batches = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dist_loader):\n",
    "        student_input_ids = batch[\"student_input_ids\"].to(device)\n",
    "        student_attention_mask = batch[\"student_attention_mask\"].to(device)\n",
    "\n",
    "        teacher_input_ids = batch[\"teacher_input_ids\"].to(device)\n",
    "        teacher_attention_mask = batch[\"teacher_attention_mask\"].to(device)\n",
    "\n",
    "        try:\n",
    "            # Teacher forward\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(\n",
    "                    input_ids=teacher_input_ids,\n",
    "                    attention_mask=teacher_attention_mask\n",
    "                )\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "\n",
    "            # Student forward\n",
    "            student_outputs = student_model(\n",
    "                input_ids=student_input_ids,\n",
    "                attention_mask=student_attention_mask\n",
    "            )\n",
    "            student_logits = student_outputs.logits\n",
    "\n",
    "            # --- Next-token distillation ---\n",
    "            min_len = min(student_logits.size(1)-1, teacher_logits.size(1)-1)\n",
    "            student_logits_next = student_logits[:, :min_len, :]\n",
    "            teacher_logits_next = teacher_logits[:, 1:min_len+1, :]\n",
    "\n",
    "\n",
    "            # Flatten and compute loss\n",
    "            loss = distillation_loss(\n",
    "                student_logits_next.reshape(-1, student_logits_next.size(-1)),\n",
    "                teacher_logits_next.reshape(-1, teacher_logits_next.size(-1)),\n",
    "            )\n",
    "\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(student_model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            valid_batches += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "            progress_bar.update(1)\n",
    "            continue\n",
    "\n",
    "    if valid_batches > 0:\n",
    "        print(f\"Epoch {epoch+1}, Avg Loss: {total_loss/valid_batches:.4f} ({valid_batches} batches)\")\n",
    "\n",
    "print(\"\\n✅ Distillation training complete!\")\n",
    "\n",
    "# --- 5. Save Distilled Model ---\n",
    "output_dir = \"./distilled_llama_1b_jailbreak_safety\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "student_model.save_pretrained(output_dir)\n",
    "student_tokenizer.save_pretrained(output_dir)\n",
    "print(\"✅ Distilled model saved.\")\n",
    "\n",
    "# --- 6. Example Inference ---\n",
    "student_model.eval()\n",
    "test_prompt = \"Explain the history of cryptography\"\n",
    "input_ids = student_tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=128).input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_output = student_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        pad_token_id=student_tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "decoded_output = student_tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(f\"\\n🎓 Distilled Student Output:\\n{decoded_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565aed8b",
   "metadata": {},
   "source": [
    "## Pipeline Steps\n",
    "1. Load student and teacher models (with quantization and CUDA)\n",
    "2. Prepare and tokenize jailbreak dataset\n",
    "3. Define KD loss and custom Trainer\n",
    "4. Train student model with KD\n",
    "5. Evaluate and save the distilled student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64a9dd6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m test_prompt = \u001b[33m\"\u001b[39m\u001b[33mHello\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m input_ids = \u001b[43mstudent_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      5\u001b[39m     generated_output = student_model.generate(\n\u001b[32m      6\u001b[39m         input_ids,\n\u001b[32m      7\u001b[39m         max_new_tokens=\u001b[32m64\u001b[39m,\n\u001b[32m      8\u001b[39m         do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      9\u001b[39m         pad_token_id=student_tokenizer.pad_token_id\n\u001b[32m     10\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"Hello\"\n",
    "input_ids = student_tokenizer(test_prompt, return_tensors=\"pt\", truncation=True, max_length=128).input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_output = student_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        pad_token_id=student_tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "decoded_output = student_tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(f\"\\n🎓 Distilled Student Output:\\n{decoded_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jailbreak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
