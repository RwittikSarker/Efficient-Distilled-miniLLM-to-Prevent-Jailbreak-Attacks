{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312a2f47",
   "metadata": {},
   "source": [
    "# Knowledge Distillation Pipeline\n",
    "This notebook demonstrates knowledge distillation using LoRA adapters and your Llama models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bcc25f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: safetensors in c:\\users\\t2430392\\appdata\\roaming\\python\\python312\\site-packages (0.5.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\T2430392\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Install required packages and import libraries\n",
    "%pip install --upgrade ipywidgets jupyter --quiet\n",
    "%pip install peft optuna safetensors --quiet\n",
    "%pip install safetensors\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from safetensors.torch import safe_open\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import optuna\n",
    "import copy\n",
    "\n",
    "# Basic settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 2\n",
    "batch_size = 8\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6949173d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Student Model (Llama 1B) ---\n",
      "--- Student Model Loaded ---\n",
      "--- Loading Teacher Model (Llama 8B) ---\n",
      "--- Student Model Loaded ---\n",
      "--- Loading Teacher Model (Llama 8B) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.52s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Teacher Model Loaded ---\n",
      "Loaded student and teacher models\n"
     ]
    }
   ],
   "source": [
    "# Load student and teacher models using your custom loaders (keeps code modular)\n",
    "from llama_1b import load_llama_1b\n",
    "from llama_8b import load_llama_8b\n",
    "\n",
    "student_model, tokenizer = load_llama_1b()\n",
    "teacher_model, _ = load_llama_8b()\n",
    "\n",
    "student_model.to(device)\n",
    "teacher_model.to(device)\n",
    "\n",
    "# Ensure tokenizer has pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "student_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print('Loaded student and teacher models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e1cdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA attached to student model\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA for the student model\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.2,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    ")\n",
    "student_model = get_peft_model(student_model, lora_config)\n",
    "print('LoRA attached to student model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "623902cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 2638 train examples, 1319 eval examples\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare data\n",
    "train_data = load_dataset('openai/gsm8k', 'socratic', split='train').select(range(2638))\n",
    "val_data = load_dataset('openai/gsm8k', 'socratic', split='test')\n",
    "print('Data loaded:', len(train_data), 'train examples,', len(val_data), 'eval examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "369d9c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing teacher logits for 2638 examples -> saving to 'teacher_logits'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFinished generating and saving teacher logits.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- Execute the function ---\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# This will create a 'teacher_logits' directory with one file per training example.\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# It uses your loaded teacher_model.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mcompute_and_save_teacher_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mteacher_logits\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mcompute_and_save_teacher_logits\u001b[39m\u001b[34m(teacher_model, tokenizer, dataset, out_dir, device, max_examples)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputing teacher logits for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_examples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m examples -> saving to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGenerating Teacher Logits\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m     22\u001b[39m         example = dataset[i]\n\u001b[32m     23\u001b[39m         question = example.get(\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
      "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# Cell 1: Generate and Save Teacher Logits\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def compute_and_save_teacher_logits(teacher_model, tokenizer, dataset, out_dir='teacher_logits', device=None, max_examples=None):\n",
    "    \"\"\"\n",
    "    Run the teacher model on each example and save the logits for the answer tokens.\n",
    "    Saves one .pt file per example to avoid high RAM usage.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    if device is None:\n",
    "        device = next(teacher_model.parameters()).device\n",
    "\n",
    "    teacher_model.eval()\n",
    "    num_examples = len(dataset) if max_examples is None else min(len(dataset), max_examples)\n",
    "    \n",
    "    print(f\"Computing teacher logits for {num_examples} examples -> saving to '{out_dir}'\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_examples), desc=\"Generating Teacher Logits\"):\n",
    "            example = dataset[i]\n",
    "            question = example.get('question')\n",
    "            answer = example.get('answer')\n",
    "\n",
    "            if not question or not answer:\n",
    "                # Save an empty tensor for malformed examples to maintain order\n",
    "                torch.save(torch.empty((0, teacher_model.config.vocab_size)), os.path.join(out_dir, f\"{i:08d}.pt\"))\n",
    "                continue\n",
    "\n",
    "            # Concatenate question and answer to match how the model sees the full sequence\n",
    "            concat_text = question + (tokenizer.eos_token or '') + answer\n",
    "            inputs = tokenizer(concat_text, truncation=True, padding=False, return_tensors='pt').to(device)\n",
    "            \n",
    "            # Get logits for the entire sequence\n",
    "            outputs = teacher_model(**inputs)\n",
    "            logits = outputs.logits.cpu()  # Shape: (1, seq_len, vocab_size)\n",
    "\n",
    "            # Isolate the logits that correspond to the 'answer' part\n",
    "            answer_tokens = tokenizer(answer, truncation=True, padding=False, return_tensors='pt')['input_ids']\n",
    "            answer_len = answer_tokens.size(1)\n",
    "            \n",
    "            if answer_len > 0:\n",
    "                # The answer logits are the last 'answer_len' tokens of the full sequence logits\n",
    "                answer_logits = logits[0, -answer_len:, :].contiguous()\n",
    "            else:\n",
    "                answer_logits = torch.empty((0, logits.size(-1)), dtype=logits.dtype)\n",
    "\n",
    "            # Save the tensor for this specific example\n",
    "            torch.save(answer_logits, os.path.join(out_dir, f\"{i:08d}.pt\"))\n",
    "\n",
    "    print('Finished generating and saving teacher logits.')\n",
    "\n",
    "# --- Execute the function ---\n",
    "# This will create a 'teacher_logits' directory with one file per training example.\n",
    "# It uses your loaded teacher_model.\n",
    "compute_and_save_teacher_logits(teacher_model, tokenizer, train_data, out_dir='teacher_logits', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6b562",
   "metadata": {},
   "source": [
    "### Summary of the Plan\n",
    "\n",
    "1.  **Generate Teacher Logits**: We will use your existing `teacher_model` to process the `train_data` and save the output logits. This creates the missing `safetensors` file that the research pipeline needs. We will save one file per example to avoid using too much memory.\n",
    "2.  **Load Teacher Logits**: We will load these newly created logits.\n",
    "3.  **Run the Pipeline**: With the logits loaded, the existing training cell from the research pipeline should now work correctly.\n",
    "4.  **Diagnostic (Optional)**: I've included a cell to run a check on a single example. This is extremely useful for debugging and verifying that the shapes and loss values make sense before starting a long training run.\n",
    "\n",
    "Run the following cells in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88592090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading teacher logits from ../llama-8b-gsm8k-tensors.safetensors\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: \"../llama-8b-gsm8k-tensors.safetensors\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoading teacher logits from\u001b[39m\u001b[33m'\u001b[39m, teacher_logits_path)\n\u001b[32m      5\u001b[39m teacher_logits_L = []\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_logits_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      7\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(f.keys())\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFound\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(keys), \u001b[33m'\u001b[39m\u001b[33mlogits entries (keys). Ensure ordering matches your train_data\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file or directory: \"../llama-8b-gsm8k-tensors.safetensors\""
     ]
    }
   ],
   "source": [
    "# Cell 2: Load the newly created teacher logits\n",
    "\n",
    "import glob\n",
    "\n",
    "def load_teacher_logits_from_files(dirpath='teacher_logits', max_examples=None):\n",
    "    \"\"\"\n",
    "    Loads the per-example teacher logits from the specified directory.\n",
    "    Ensures they are loaded in the correct order to match the dataset.\n",
    "    \"\"\"\n",
    "    # Sort files numerically to ensure order matches dataset index\n",
    "    files = sorted(glob.glob(os.path.join(dirpath, '*.pt')))\n",
    "    \n",
    "    if max_examples is not None:\n",
    "        files = files[:max_examples]\n",
    "        \n",
    "    print(f\"Loading {len(files)} teacher logit tensors from '{dirpath}'...\")\n",
    "    \n",
    "    # Load each tensor into a list\n",
    "    teacher_logits_list = [torch.load(p) for p in tqdm(files, desc=\"Loading Logits\")]\n",
    "    \n",
    "    print(f\"Successfully loaded {len(teacher_logits_list)} logit tensors.\")\n",
    "    return teacher_logits_list\n",
    "\n",
    "# --- Execute the function ---\n",
    "# This replaces the old safetensors loading logic with our file-based approach.\n",
    "teacher_logits_L = load_teacher_logits_from_files('teacher_logits', max_examples=len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd520ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: get teacher logits on-the-fly for a batch (no safetensors)\n",
    "def get_teacher_logits_for_batch(teacher_model, tokenizer, questions, answers, device, max_length=512):\n",
    "    teacher_model.eval()\n",
    "    concat_texts = [q + (tokenizer.eos_token or \"\") + a for q, a in zip(questions, answers)]\n",
    "    encoded = tokenizer(concat_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    ans_enc = tokenizer(answers, truncation=True, padding=False, return_tensors=\"pt\")\n",
    "    answer_lens = [int(x.size(0)) for x in ans_enc[\"input_ids\"]]\n",
    "\n",
    "    batch_logits = []\n",
    "    for i, ans_len in enumerate(answer_lens):\n",
    "        if ans_len == 0:\n",
    "            v = torch.zeros((0, logits.size(-1)), device=logits.device, dtype=logits.dtype)\n",
    "            batch_logits.append(v)\n",
    "            continue\n",
    "        seq_len = int(attention_mask[i].sum().item())\n",
    "        start_idx = seq_len - ans_len\n",
    "        if start_idx < 0:\n",
    "            start_idx = max(0, seq_len - ans_len)\n",
    "        v = logits[i, start_idx:seq_len, :].cpu()\n",
    "        batch_logits.append(v)\n",
    "\n",
    "    max_ans_len = max([v.size(0) for v in batch_logits]) if batch_logits else 0\n",
    "    if max_ans_len == 0:\n",
    "        return torch.empty((len(batch_logits), 0, logits.size(-1))).to(device)\n",
    "\n",
    "    padded = torch.zeros((len(batch_logits), max_ans_len, logits.size(-1)), dtype=batch_logits[0].dtype)\n",
    "    for i, v in enumerate(batch_logits):\n",
    "        L = v.size(0)\n",
    "        if L > 0:\n",
    "            padded[i, :L, :] = v\n",
    "\n",
    "    return padded.to(device)\n",
    "\n",
    "print('Teacher logits helper ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe218230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Distillation Loss\n",
    "class KnowledgeDistillationLoss(nn.Module):\n",
    "    def __init__(self, temperature=1.0, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        loss_hard = self.criterion(student_logits, labels)\n",
    "        teacher_log_probs = F.log_softmax(teacher_logits / self.temperature, dim=1)\n",
    "        student_probs = F.softmax(student_logits / self.temperature, dim=1)\n",
    "        loss_soft = F.kl_div(teacher_log_probs, student_probs, reduction='batchmean', log_target=False) * (self.temperature ** 2)\n",
    "        return self.alpha * loss_hard + (1.0 - self.alpha) * loss_soft\n",
    "\n",
    "print('KD loss defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Updated Training Function (Handles padding and alignment)\n",
    "\n",
    "def train_model(model, teacher_logits_L, data, tokenizer, optimizer, scheduler, kd_loss, num_epochs, device, batch_size=1):\n",
    "    model.train()\n",
    "    num_batches = len(data) // batch_size + int(len(data) % batch_size != 0)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        progress_bar = tqdm(range(num_batches), desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "\n",
    "        for batch_idx in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(data))\n",
    "\n",
    "            batch_data = data[start_idx:end_idx]\n",
    "            batch_teacher_logits = teacher_logits_L[start_idx:end_idx]\n",
    "            \n",
    "            questions = [ex['question'] for ex in batch_data]\n",
    "            answers = [ex['answer'] for ex in batch_data]\n",
    "\n",
    "            # Tokenize questions and answers separately for better control\n",
    "            inputs = tokenizer(questions, return_tensors='pt', padding=True, truncation=True, max_length=256).to(device)\n",
    "            labels = tokenizer(answers, return_tensors='pt', padding=True, truncation=True, max_length=256).to(device)\n",
    "\n",
    "            # Get student logits\n",
    "            outputs = model(**inputs, labels=labels['input_ids'])\n",
    "            student_logits = outputs.logits\n",
    "\n",
    "            # Pad the teacher logits to match the batch's max sequence length\n",
    "            # This is critical for alignment.\n",
    "            max_len = labels['input_ids'].size(1)\n",
    "            padded_teacher_logits = torch.full(\n",
    "                (len(batch_teacher_logits), max_len, student_logits.size(-1)),\n",
    "                fill_value=0.0, # Use a neutral fill value\n",
    "                device=device,\n",
    "                dtype=student_logits.dtype\n",
    "            )\n",
    "            for i, t in enumerate(batch_teacher_logits):\n",
    "                len_t = min(t.size(0), max_len)\n",
    "                if len_t > 0:\n",
    "                    padded_teacher_logits[i, :len_t, :] = t[:len_t].to(device)\n",
    "\n",
    "            # Align sequence lengths for all tensors before flattening\n",
    "            seq_len = min(student_logits.size(1), padded_teacher_logits.size(1), labels['input_ids'].size(1))\n",
    "            \n",
    "            student_logits_aligned = student_logits[:, :seq_len, :]\n",
    "            teacher_logits_aligned = padded_teacher_logits[:, :seq_len, :]\n",
    "            labels_aligned = labels['input_ids'][:, :seq_len]\n",
    "\n",
    "            # Flatten for loss calculation\n",
    "            student_flat = student_logits_aligned.contiguous().view(-1, student_logits.size(-1))\n",
    "            teacher_flat = teacher_logits_aligned.contiguous().view(-1, student_logits.size(-1))\n",
    "            labels_flat = labels_aligned.contiguous().view(-1)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = kd_loss(student_flat, teacher_flat, labels_flat)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    return model\n",
    "\n",
    "print('Train function updated to handle padding of file-based teacher logits.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, validation_data, tokenizer, device, max_eval=200):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for example in validation_data[:max_eval]:\n",
    "            inputs = tokenizer(example['question'], truncation=True, padding=True, max_length=256, return_tensors='pt').to(device)\n",
    "            labels = tokenizer(example['answer'], truncation=True, padding=True, max_length=256, return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            student_logits = outputs.logits\n",
    "\n",
    "            seq_len = min(student_logits.size(1), labels.size(1))\n",
    "            student_logits = student_logits[:, :seq_len, :]\n",
    "            labels = labels[:, :seq_len]\n",
    "\n",
    "            student_flat = student_logits.contiguous().view(-1, student_logits.size(-1))\n",
    "            labels_flat = labels.view(-1)\n",
    "\n",
    "            loss = F.cross_entropy(student_flat, labels_flat, ignore_index=tokenizer.pad_token_id, reduction='sum')\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += (labels_flat != tokenizer.pad_token_id).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('nan')\n",
    "    return avg_loss\n",
    "\n",
    "print('Evaluate function ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler setup\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "lora_params = [p for n, p in student_model.named_parameters() if 'lora' in n]\n",
    "base_params = [p for n, p in student_model.named_parameters() if 'lora' not in n]\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': base_params, 'weight_decay': 0.0},\n",
    "    {'params': lora_params, 'weight_decay': 1e-2},\n",
    "]\n",
    "optimizer = torch.optim.SGD(optimizer_grouped_parameters, lr=5e-6, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_data))\n",
    "kd_loss = KnowledgeDistillationLoss(temperature=5.94, alpha=0.61)\n",
    "\n",
    "print('Optimizer and scheduler ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d8d4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trained_model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkd_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m val_loss = evaluate_model(trained_model, val_data, tokenizer, device)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mValidation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, teacher_model, data, tokenizer, optimizer, scheduler, kd_loss, num_epochs, device, batch_size, teacher_max_length)\u001b[39m\n\u001b[32m      3\u001b[39m num_batches = \u001b[38;5;28mlen\u001b[39m(data) // batch_size + \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) % batch_size != \u001b[32m0\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     progress_bar = \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[32m      9\u001b[39m         start_idx = batch_idx * batch_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
      "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# Train and evaluate (run this cell to start training)\n",
    "# Ensure we pass the precomputed teacher logits list `teacher_logits_L` (not the teacher model)\n",
    "trained_model = train_model(student_model, teacher_logits_L, train_data, tokenizer, optimizer, scheduler, kd_loss, num_epochs=num_epochs, device=device, batch_size=batch_size)\n",
    "val_loss = evaluate_model(trained_model, val_data, tokenizer, device)\n",
    "print(f'Validation loss: {val_loss}')\n",
    "\n",
    "# Optional: save the LoRA weights only\n",
    "student_model.save_pretrained('llama1b-lora-kd', max_shard_size='5GB')\n",
    "print('Saved student LoRA weights to llama1b-lora-kd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdf85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 (Optional but Recommended): Single-Example Diagnostic\n",
    "\n",
    "def kd_single_example_check(student_model, tokenizer, teacher_tensor, example, device, kd_loss_obj):\n",
    "    \"\"\"\n",
    "    Run a single example through the student and teacher to check shapes and loss components.\n",
    "    This is invaluable for debugging.\n",
    "    \"\"\"\n",
    "    student_model.train()\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "    \n",
    "    # Tokenize inputs and labels\n",
    "    inputs = tokenizer(question, return_tensors='pt', padding=True, truncation=True, max_length=256).to(device)\n",
    "    labels = tokenizer(answer, return_tensors='pt', padding=True, truncation=True, max_length=256).to(device)\n",
    "    \n",
    "    # Get student logits\n",
    "    with torch.no_grad():\n",
    "        outputs = student_model(**inputs, labels=labels['input_ids'])\n",
    "        student_logits = outputs.logits\n",
    "\n",
    "    # Align teacher tensor\n",
    "    teacher_t = teacher_tensor.to(device)\n",
    "    \n",
    "    # Align sequence lengths\n",
    "    seq_len = min(student_logits.size(1), teacher_t.size(0), labels['input_ids'].size(1))\n",
    "    \n",
    "    student_logits_aligned = student_logits[:, :seq_len, :]\n",
    "    teacher_logits_aligned = teacher_t[:seq_len, :].unsqueeze(0) # Add batch dim\n",
    "    labels_aligned = labels['input_ids'][:, :seq_len]\n",
    "\n",
    "    # Flatten for loss\n",
    "    V = student_logits.size(-1)\n",
    "    student_flat = student_logits_aligned.contiguous().view(-1, V)\n",
    "    teacher_flat = teacher_logits_aligned.contiguous().view(-1, V)\n",
    "    labels_flat = labels_aligned.contiguous().view(-1)\n",
    "\n",
    "    print('--- Single-Example Diagnostic ---')\n",
    "    print(f\"Shapes (student, teacher, labels): {student_flat.shape}, {teacher_flat.shape}, {labels_flat.shape}\")\n",
    "\n",
    "    # Calculate loss components\n",
    "    loss_hard = F.cross_entropy(student_flat, labels_flat, ignore_index=tokenizer.pad_token_id)\n",
    "    \n",
    "    T = kd_loss_obj.temperature\n",
    "    alpha = kd_loss_obj.alpha\n",
    "    \n",
    "    teacher_log_probs = F.log_softmax(teacher_flat / T, dim=1)\n",
    "    student_probs = F.softmax(student_flat / T, dim=1)\n",
    "    loss_soft = F.kl_div(teacher_log_probs, student_probs, reduction='batchmean', log_target=True) * (T ** 2)\n",
    "    \n",
    "    total_loss = alpha * loss_hard + (1.0 - alpha) * loss_soft\n",
    "\n",
    "    print(f\"Loss Components: Hard={loss_hard.item():.4f}, Soft(KL)={loss_soft.item():.4f}, Total KD={total_loss.item():.4f}\")\n",
    "    print('--- End Diagnostic ---')\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# --- Execute the diagnostic on the first training example ---\n",
    "if train_data and teacher_logits_L:\n",
    "    print(\"Running diagnostic on the first training example...\")\n",
    "    kd_single_example_check(student_model, tokenizer, teacher_logits_L[0], train_data[0], device, kd_loss)\n",
    "else:\n",
    "    print(\"Skipping diagnostic: data or logits not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d5e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a788f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede8d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de1b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d283d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129fd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48021e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5fc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
