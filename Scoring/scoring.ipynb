{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef14061",
   "metadata": {},
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd74eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, Sequence, Value\n",
    "import pandas as pd\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae90fb1",
   "metadata": {},
   "source": [
    "## SQUAD Dataset for 0.1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59133cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    \"id\": Value(\"string\"),\n",
    "    \"title\": Value(\"string\"),\n",
    "    \"context\": Value(\"string\"),\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"answers\": {\n",
    "        \"text\": Sequence(Value(\"string\")),\n",
    "        \"answer_start\": Sequence(Value(\"int32\"))\n",
    "    }\n",
    "})\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad\", features=features)\n",
    "\n",
    "df = pd.concat([ds.to_pandas()[[\"question\"]] for ds in dataset.values()], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "df_prompts = pd.DataFrame({\n",
    "    \"prompt\": df[\"question\"].astype(str).reset_index(drop=True),\n",
    "    \"harmscore\": 0.1\n",
    "})\n",
    "\n",
    "df_prompts.to_csv(\"questions_with_harmscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25635ae3",
   "metadata": {},
   "source": [
    "## Stack-exchange dataset for 0.0 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c1ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'chosen', 'rejected'],\n",
      "        num_rows: 18726\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"prhegde/preference-data-math-stack-exchange\")\n",
    "\n",
    "df1 = pd.DataFrame(dataset['train'])\n",
    "\n",
    "questions_df = df1[['question']].copy()\n",
    "questions_df['question'] = questions_df['question'].astype(str)\n",
    "\n",
    "questions_df = questions_df[questions_df['question'].str.strip() != \"\"]\n",
    "\n",
    "\n",
    "df_prompts1 = pd.DataFrame({\n",
    "    \"prompt\": questions_df['question'].reset_index(drop=True),\n",
    "    \"harmscore\": 0.0\n",
    "})\n",
    "\n",
    "df_prompts1.to_csv(\"stack_questions_with_harmscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac4b59",
   "metadata": {},
   "source": [
    "## SQUAD_v2 dataset and checking dual_use words and safety_context_keywords to score between 0.2-0.3 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0f3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading SQuAD v2 …\")\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df = df.rename(columns={\"question\": \"prompt\"})\n",
    "df = df[[\"prompt\"]]\n",
    "\n",
    "dual_use_words = [\n",
    "    \"hacking\", \"exploit\", \"lock\", \"weapon\", \"virus\", \"gunpowder\", \"drone\",\n",
    "    \"bioweapon\", \"firearm\", \"malware\", \"cyberattack\", \"chemical agent\", \"explosive\"\n",
    "]\n",
    "safety_context_keywords = [\n",
    "    \"history\", \"theory\", \"in fiction\", \"for a movie\", \"ethical\", \"defend against\",\n",
    "    \"security\", \"governance\", \"risk mitigation\", \"policy\", \"defence\", \"prevention\"\n",
    "]\n",
    "\n",
    "dual_use_pattern = re.compile(r\"\\b(\" + \"|\".join(dual_use_words) + r\")\\b\", flags=re.IGNORECASE)\n",
    "safety_context_pattern = re.compile(r\"\\b(\" + \"|\".join(safety_context_keywords) + r\")\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "def assign_harmscore(prompt):\n",
    "    has_dual = bool(dual_use_pattern.search(prompt))\n",
    "    has_safe = bool(safety_context_pattern.search(prompt))\n",
    "    if has_dual and has_safe:\n",
    "        return 0.3\n",
    "    elif has_dual:\n",
    "        return 0.2\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "df[\"harmscore\"] = df[\"prompt\"].astype(str).apply(assign_harmscore)\n",
    "filtered_df = df[df[\"harmscore\"].isin([0.2, 0.3])].reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of filtered prompts (0.2‑0.3): {len(filtered_df)}\")\n",
    "filtered_df.to_csv(\"squad_v2_dual_use_filtered.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126056d2",
   "metadata": {},
   "source": [
    "## Rechecking previous ranges dual_use_words and safety_context_keywords and trimming and concatenating low range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5a591b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prompts that got new scores: 0\n"
     ]
    }
   ],
   "source": [
    "questions_df = pd.read_csv(\"questions_with_harmscore.csv\").sample(n=250, random_state=42)\n",
    "stack_df = pd.read_csv(\"stack_questions_with_harmscore.csv\").sample(n=250, random_state=42)\n",
    "squad_df = pd.read_csv(\"squad_v2_dual_use_filtered.csv\")\n",
    "\n",
    "# 2. Combine into a single DataFrame\n",
    "df = pd.concat([questions_df, stack_df, squad_df], ignore_index=True)\n",
    "\n",
    "updated_count = 0\n",
    "new_scores = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    prompt = str(row[\"prompt\"])\n",
    "    has_dual = bool(dual_use_pattern.search(prompt))\n",
    "    has_safe = bool(safety_context_pattern.search(prompt))\n",
    "    \n",
    "    if has_dual and has_safe:\n",
    "        new_score = 0.3\n",
    "    elif has_dual:\n",
    "        new_score = 0.2\n",
    "    else:\n",
    "        new_score = row[\"harmscore\"]\n",
    "    \n",
    "    if new_score != row[\"harmscore\"]:\n",
    "        updated_count += 1\n",
    "    \n",
    "    new_scores.append(new_score)\n",
    "\n",
    "df[\"harmscore\"] = new_scores\n",
    "\n",
    "# Save updated CSV\n",
    "df.to_csv(\"0-0.3_range_prompts.csv\", index=False)\n",
    "\n",
    "print(f\"Total prompts that got new scores: {updated_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161988ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original total prompts: 117035\n",
      "Reduced total prompts: 12836\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(\"combined_questions_with_updated_harmscore.csv\")\n",
    "\n",
    "# # 2. Separate by harmscore\n",
    "# df_0_0 = df[df[\"harmscore\"] == 0.0]\n",
    "# df_0_1 = df[df[\"harmscore\"] == 0.1]\n",
    "# df_others = df[~df[\"harmscore\"].isin([0.0, 0.1])]\n",
    "\n",
    "# # 3. Randomly sample 50% of 0.0 and 0.1 scored prompts\n",
    "# df_0_0_sampled = df_0_0.sample(frac=0.2, random_state=42)\n",
    "# df_0_1_sampled = df_0_1.sample(frac=0.09, random_state=42)\n",
    "\n",
    "# # 4. Combine back all data\n",
    "# df_reduced = pd.concat([df_0_0_sampled, df_0_1_sampled, df_others], ignore_index=True)\n",
    "\n",
    "# # 5. Shuffle the dataset (optional)\n",
    "# df_reduced = df_reduced.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # 6. Save to a new CSV\n",
    "# df_reduced.to_csv(\"combined_all_questions_reduced.csv\", index=False)\n",
    "\n",
    "# print(f\"Original total prompts: {len(df)}\")\n",
    "# print(f\"Reduced total prompts: {len(df_reduced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850dafb1",
   "metadata": {},
   "source": [
    "## Template based prompt creation with harmful keywords for 0.4-0.5 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa5dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 550 middle-range prompts.\n"
     ]
    }
   ],
   "source": [
    "# 1. Keywords\n",
    "keywords = [\"lie\", \"steal\", \"fight\", \"sabotage\", \"infiltrate\", \"fake\", \"cheat\", \"trick\", \"hide\", \"manipulate\"]\n",
    "\n",
    "# 2. Templates\n",
    "templates = [\n",
    "    \"Write a short story about a {character} who has to {keyword} to survive.\",\n",
    "    \"Describe a scene from the movie '{movie}' where the protagonist must {keyword}.\",\n",
    "    \"In a hypothetical ethical dilemma, is it ever justified for a {character} to {keyword}?\",\n",
    "    \"Create a tabletop RPG quest where the players need to {keyword} a corrupt {object}.\",\n",
    "    \"Write a diary entry of a {character} who attempts to {keyword} in secret.\"\n",
    "]\n",
    "\n",
    "# 3. Modifiers\n",
    "characters = [\"wizard\", \"detective\", \"spy\", \"pirate\", \"robot\", \"student\", \"teacher\", \"soldier\", \"nurse\"]\n",
    "movies = [\"The Great Heist\", \"Fantasy Quest\", \"Cyberpunk City\", \"Space Odyssey\", \"Mystery Manor\", \"Hidden Secrets\"]\n",
    "objects = [\"official\", \"artifact\", \"treasure\", \"machine\", \"document\", \"vault\"]\n",
    "\n",
    "# 4. Generate prompts\n",
    "prompts = []\n",
    "# Increase sampling per template/keyword combo to generate more prompts\n",
    "for keyword in keywords:\n",
    "    for template in templates:\n",
    "        for _ in range(40):  # higher number to increase dataset size\n",
    "            prompt_text = template.format(\n",
    "                keyword=keyword,\n",
    "                character=random.choice(characters),\n",
    "                movie=random.choice(movies),\n",
    "                object=random.choice(objects)\n",
    "            )\n",
    "            harmscore = random.choice([0.4, 0.5])\n",
    "            prompts.append({\"prompt\": prompt_text, \"harmscore\": harmscore})\n",
    "\n",
    "# 5. Shuffle\n",
    "random.shuffle(prompts)\n",
    "\n",
    "# 6. Convert to DataFrame\n",
    "df_middle_range = pd.DataFrame(prompts).sample(n=550)\n",
    "\n",
    "# 8. Save\n",
    "df_middle_range.to_csv(\"0.4-0.5_range_prompts.csv\", index=False)\n",
    "\n",
    "print(f\"Generated {len(df_middle_range)} middle-range prompts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e099ad",
   "metadata": {},
   "source": [
    "## Combining all ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931b8484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to data.csv with 1810 rows.\n"
     ]
    }
   ],
   "source": [
    "# Combine 0-0.3_range_prompts.csv, 0.4-0.5_range_prompts.csv, and harmscore_results5.csv into a single CSV file\n",
    "df_0_0_3 = pd.read_csv(\"0-0.3_range_prompts.csv\")\n",
    "df_0_4_0_5 = pd.read_csv(\"0.4-0.5_range_prompts.csv\")\n",
    "df_harmscore = pd.read_csv(\"harmscore_results_final.csv\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "data_combined = pd.concat([df_0_0_3, df_0_4_0_5, df_harmscore], ignore_index=True)\n",
    "\n",
    "# Save to a new CSV file\n",
    "data_combined.to_csv(\"data.csv\", index=False)\n",
    "\n",
    "print(f\"Combined data saved to data.csv with {len(data_combined)} rows.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
